{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_seq_to_seq.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlNpj/MpkaC7DZbDkyuIHx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swetha-rana/Assignment_3/blob/main/rnn_seq_to_seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cells installs the necessary modules"
      ],
      "metadata": {
        "id": "gkuFi-ta8nvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "O3i9fiJ38m3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cell imports the necessary modules"
      ],
      "metadata": {
        "id": "fs_zq4Y_8K03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BME0keAv8FY4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cell downloads the dataset and untars it"
      ],
      "metadata": {
        "id": "soB70qN69k2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xvf dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "21JXJTj-8J-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing cell"
      ],
      "metadata": {
        "id": "pbjOLDTiBOdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(path,ip_token=None,ip_len=None,output_token=None,output_len=None):\n",
        "  \n",
        "  ip_transcription= []\n",
        "  output_transcription= []\n",
        "  \n",
        "  df= pd.read_csv(path,names=[\"1\", \"2\",\"3\"],sep=\"\\t\").astype(str)\n",
        "  if ip_token is None:\n",
        "      df=df.sample(frac=1)\n",
        "  for index, row in df.iterrows():\n",
        "      ip_text=row['2']\n",
        "      op_text= row['1']\n",
        "      if ip_text=='</s>' or op_text =='</s>':\n",
        "        continue\n",
        "      op_text= \"\\t\" + op_text + \"\\n\"\n",
        "      ip_transcription.append(ip_text)\n",
        "      output_transcription.append(op_text)\n",
        "  \n",
        "  if ip_token is None:\n",
        "    ip_token= tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    ip_token.fit_on_texts(ip_transcription)\n",
        "  input_text= ip_token.texts_to_sequences(ip_transcription)\n",
        "  input_text= tf.keras.preprocessing.sequence.pad_sequences(input_text,padding='post')\n",
        "\n",
        "  if output_token is None:\n",
        "    output_token= tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    output_token.fit_on_texts(output_transcription)\n",
        "\n",
        "  output_text= output_token.texts_to_sequences(output_transcription)\n",
        "  output_text= tf.keras.preprocessing.sequence.pad_sequences(output_text,padding='post')\n",
        "\n",
        "  if ip_len is not None and output_len is not None:\n",
        "      input_text=tf.concat([input_text,tf.zeros((input_text.shape[0],ip_len-input_text.shape[1]))],axis=1)\n",
        "      output_text=tf.concat([output_text,tf.zeros((output_text.shape[0],output_len-output_text.shape[1]))],axis=1)\n",
        "\n",
        "  return ip_transcription,input_text,ip_token,output_transcription,output_text,output_token"
      ],
      "metadata": {
        "id": "qBDjm53fBNJ8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ip_transcription,train_input_text,train_ip_token,train_output_transcription,train_output_text,train_output_token=data_preprocessing(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\")\n",
        "\n",
        "test_ip_transcription,test_input_text,test_ip_token,test_output_transcription,test_output_text,test_output_token=data_preprocessing(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\",train_ip_token,train_input_text.shape[1],train_output_token,train_output_text.shape[1])\n",
        "\n",
        "val_ip_transcription,val_input_text,val_ip_token,val_output_transcription,val_output_text,val_output_token=data_preprocessing(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\",train_ip_token,train_input_text.shape[1],train_output_token,train_output_text.shape[1])\n",
        "\n",
        "encoder_tokens = len(train_ip_token.word_index)+1\n",
        "\n",
        "encoder_seq_length =  train_input_text.shape[1]\n",
        "\n",
        "decoder_tokens = len(train_output_token.word_index)+1\n",
        "\n",
        "decoder_seq_length = train_output_text.shape[1]\n",
        "\n",
        "index_to_char_input = dict((train_ip_token.word_index[key], key) for key in train_ip_token.word_index.keys())\n",
        "\n",
        "index_to_char_target = dict((train_output_token.word_index[key], key) for key in train_output_token.word_index.keys())"
      ],
      "metadata": {
        "id": "mxMx8vYNCjIp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_seq_model(rnn_type,embed_dim,encoder_layers,decoder_layers,dropout):\n",
        "  \n",
        "  encoder_inputs = keras.Input(shape=(encoder_seq_length))\n",
        "  embed = keras.layers.Embedding(encoder_tokens, embed_dim)(encoder_inputs)\n",
        "  last_encoder=None\n",
        "  \n",
        "  if rnn_type=='RNN':\n",
        "    for i in range(encoder_layers-1):      \n",
        "      encoder = keras.layers.SimpleRNN(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        enc_out = encoder(embed)\n",
        "      else:\n",
        "        enc_out = encoder(last_encoder)\n",
        "      last_encoder=enc_out\n",
        "    encoder = keras.layers.SimpleRNN(latent_dim, return_state=True,dropout=dropout)\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]  \n",
        "    decoder_inputs = keras.Input(shape=(decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(decoder_tokens, embed_dim)(decoder_inputs)\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.SimpleRNN(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    decoder_dense = keras.layers.Dense(decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "\n",
        "  elif rnn_type=='GRU':\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.GRU(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        enc_out = encoder(embed)\n",
        "      else:\n",
        "        enc_out = encoder(last_encoder)\n",
        "      last_encoder=enc_out\n",
        "    encoder = keras.layers.GRU(latent_dim, return_state=True,dropout=dropout)\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(decoder_tokens, embed_dim)(decoder_inputs)  \n",
        "    \n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    decoder_dense = keras.layers.Dense(decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "\n",
        "  elif rnn_type=='LSTM':\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.LSTM(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        enc_out = encoder(embed)\n",
        "      else:\n",
        "        enc_out = encoder(last_encoder)\n",
        "      last_encoder=enc_out\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True,dropout=dropout)\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state_h, state_c = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state_h, state_c = encoder(last_encoder)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(decoder_tokens, embed_dim)(decoder_inputs)  \n",
        "\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _, _ = decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    decoder_dense = keras.layers.Dense(decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "sUW0jwq7fqY4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_(model,encoder_layers,decoder_layers):\n",
        "    encoder_inputs = model.input[0]  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "    decoder_inputs =  keras.Input(shape=( 1))  \n",
        "\n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)      \n",
        "\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[] \n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)    \n",
        "    \n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "        x = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input_h)\n",
        "        decoder_states_inputs.append (decoder_state_input_c)\n",
        "        decoder_states.append (state_h_dec)\n",
        "        decoder_states.append (state_c_dec)\n",
        "    decoder_dense = model.get_layer('final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "    decoder_model = keras.Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states )\n",
        "    return encoder_model,decoder_model\n"
      ],
      "metadata": {
        "id": "DZOzMZzZ597h"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}