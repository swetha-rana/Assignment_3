{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swetha-rana/Assignment_3/blob/main/rnn_seq_to_seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the necessary modules"
      ],
      "metadata": {
        "id": "20LCQ5TFYFEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "zYVBkucYXw_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cell is responsible for downloading and extracting the data."
      ],
      "metadata": {
        "id": "Pqfi4ts6X4sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar #Download the data\n",
        "!tar -xvf dakshina_dataset_v1.0.tar #extract the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRItQLNwX1II",
        "outputId": "ab71cbe8-fe2b-4897-c470-ad65b0b70845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-08 13:56:08--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.148.128, 209.85.200.128, 209.85.234.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.148.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar.1’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   186MB/s    in 11s     \n",
            "\n",
            "2022-05-08 13:56:18 (182 MB/s) - ‘dakshina_dataset_v1.0.tar.1’ saved [2008340480/2008340480]\n",
            "\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary modules/libraries"
      ],
      "metadata": {
        "id": "-_NCA84pD1DQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyAeLYS2LDay"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data_preprocessing:** Preprocess the dataset and pads the output and also responsible for Tokenizing."
      ],
      "metadata": {
        "id": "WoXyoEwZDHT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(path,ip_token=None,ip_len=None,output_token=None,output_len=None):\n",
        "  \n",
        "  ip_transcription= []\n",
        "  output_transcription= []\n",
        "  \n",
        "  df= pd.read_csv(path,names=[\"1\", \"2\",\"3\"],sep=\"\\t\").astype(str)\n",
        "  if ip_token is None:\n",
        "      df=df.sample(frac=1)\n",
        "  for index, row in df.iterrows():\n",
        "      ip_text=row['2']\n",
        "      op_text= row['1']\n",
        "      if ip_text=='</s>' or op_text =='</s>':\n",
        "        continue\n",
        "      op_text= \"\\t\" + op_text + \"\\n\"\n",
        "      ip_transcription.append(ip_text)\n",
        "      output_transcription.append(op_text)\n",
        "  \n",
        "  if ip_token is None:\n",
        "    ip_token= tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    ip_token.fit_on_texts(ip_transcription)\n",
        "  input_text= ip_token.texts_to_sequences(ip_transcription)\n",
        "  input_text= tf.keras.preprocessing.sequence.pad_sequences(input_text,padding='post')\n",
        "\n",
        "  if output_token is None:\n",
        "    output_token= tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    output_token.fit_on_texts(output_transcription)\n",
        "\n",
        "  output_text= output_token.texts_to_sequences(output_transcription)\n",
        "  output_text= tf.keras.preprocessing.sequence.pad_sequences(output_text,padding='post')\n",
        "\n",
        "  if ip_len is not None and output_len is not None:\n",
        "      input_text=tf.concat([input_text,tf.zeros((input_text.shape[0],ip_len-input_text.shape[1]))],axis=1)\n",
        "      output_text=tf.concat([output_text,tf.zeros((output_text.shape[0],output_len-output_text.shape[1]))],axis=1)\n",
        "\n",
        "  return ip_transcription,input_text,ip_token,output_transcription,output_text,output_token"
      ],
      "metadata": {
        "id": "XV8CzrHUXE2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ip_transcription,train_input_text,train_ip_token,train_output_transcription,train_output_text,train_output_token=data_preprocessing(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\")\n",
        "\n",
        "test_ip_transcription,test_input_text,test_ip_token,test_output_transcription,test_output_text,test_output_token=data_preprocessing(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\",train_ip_token,train_input_text.shape[1],train_output_token,train_output_text.shape[1])\n",
        "\n",
        "val_ip_transcription,val_input_text,val_ip_token,val_output_transcription,val_output_text,val_output_token=data_preprocessing(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\",train_ip_token,train_input_text.shape[1],train_output_token,train_output_text.shape[1])\n",
        "\n",
        "encoder_tokens = len(train_ip_token.word_index)+1\n",
        "\n",
        "encoder_seq_length =  train_input_text.shape[1]\n",
        "\n",
        "decoder_tokens = len(train_output_token.word_index)+1\n",
        "\n",
        "decoder_seq_length = train_output_text.shape[1]\n",
        "\n",
        "index_to_char_input = dict((train_ip_token.word_index[key], key) for key in train_ip_token.word_index.keys())\n",
        "\n",
        "index_to_char_target = dict((train_output_token.word_index[key], key) for key in train_output_token.word_index.keys())"
      ],
      "metadata": {
        "id": "C0J_IAfbYugS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creats and returns the model."
      ],
      "metadata": {
        "id": "6WQ_M_UeL8ia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llw0lHWNMLdX"
      },
      "outputs": [],
      "source": [
        "def seq_seq_model(rnn_type,embed_dim,encoder_layers,decoder_layers,dropout):\n",
        "  \n",
        "  encoder_inputs = keras.Input(shape=(encoder_seq_length))\n",
        "  embed = keras.layers.Embedding(encoder_tokens, embed_dim)(encoder_inputs)\n",
        "  last_encoder=None\n",
        "  \n",
        "  if rnn_type=='RNN':\n",
        "    for i in range(encoder_layers-1):      \n",
        "      encoder = keras.layers.SimpleRNN(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        enc_out = encoder(embed)\n",
        "      else:\n",
        "        enc_out = encoder(last_encoder)\n",
        "      last_encoder=enc_out\n",
        "    encoder = keras.layers.SimpleRNN(latent_dim, return_state=True,dropout=dropout)\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]  \n",
        "    decoder_inputs = keras.Input(shape=(decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(decoder_tokens, embed_dim)(decoder_inputs)\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.SimpleRNN(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    decoder_dense = keras.layers.Dense(decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "\n",
        "  elif rnn_type=='GRU':\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.GRU(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        enc_out = encoder(embed)\n",
        "      else:\n",
        "        enc_out = encoder(last_encoder)\n",
        "      last_encoder=enc_out\n",
        "    encoder = keras.layers.GRU(latent_dim, return_state=True,dropout=dropout)\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(decoder_tokens, embed_dim)(decoder_inputs)  \n",
        "    \n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    decoder_dense = keras.layers.Dense(decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "\n",
        "  elif rnn_type=='LSTM':\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.LSTM(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        enc_out = encoder(embed)\n",
        "      else:\n",
        "        enc_out = encoder(last_encoder)\n",
        "      last_encoder=enc_out\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True,dropout=dropout)\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state_h, state_c = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state_h, state_c = encoder(last_encoder)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(decoder_tokens, embed_dim)(decoder_inputs)  \n",
        "\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _, _ = decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    decoder_dense = keras.layers.Dense(decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function performs the inference process to decode the given input"
      ],
      "metadata": {
        "id": "xZfZW0qLL129"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRNLDo7SMMhu"
      },
      "outputs": [],
      "source": [
        "def inference_(model,encoder_layers,decoder_layers):\n",
        "    encoder_inputs = model.input[0]  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "    decoder_inputs =  keras.Input(shape=( 1))  \n",
        "\n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)      \n",
        "\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[] \n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)    \n",
        "    \n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "        x = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input_h)\n",
        "        decoder_states_inputs.append (decoder_state_input_c)\n",
        "        decoder_states.append (state_h_dec)\n",
        "        decoder_states.append (state_c_dec)\n",
        "    decoder_dense = model.get_layer('final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "    decoder_model = keras.Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states )\n",
        "    return encoder_model,decoder_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**decode_and_eval** will decode the inputs and also returns the val accuracy or Test accuracy based on the test_f flag value. If test_f is False this function will return val accuracy other wise it will return the test accuracy and creates the correct_preds.txt and wrong_preds.txt."
      ],
      "metadata": {
        "id": "r8GlGK2iKzBf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvv1rtPmMaPD"
      },
      "outputs": [],
      "source": [
        "def decode_and_eval(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers,test_f):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      states_value=[states_value]\n",
        "    nl=states_value\n",
        "    for i in range(decoder_layers-1):\n",
        "      nl=nl+states_value\n",
        "    states_value=nl\n",
        "    \n",
        "\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    prev_char_index[:, 0] = train_output_token.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(decoder_seq_length):\n",
        "        out = decoder_model.predict(tuple([prev_char_index] + states_value))\n",
        "        output_probability=out[0]\n",
        "        states_value = out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          prev_char_index[j,0]=train_output_token.word_index[sampled_char]\n",
        "    \n",
        "    correct_predictions = 0\n",
        "    for t_index in range(batch_size):\n",
        "        predicted_word = predicted_words[t_index]\n",
        "        if test_f == False:\n",
        "          target_word=val_output_transcription[t_index][1:-1]\n",
        "        else:\n",
        "          target_word=test_output_transcription[t_index][1:-1]          \n",
        "        if predicted_word == target_word:\n",
        "          correct_predictions+=1\n",
        "          if test_f == True:\n",
        "            text_file = open(\"correct_preds.txt\", \"a\")\n",
        "            text_file.write(test_ip_transcription[t_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "            text_file.close()\n",
        "        else: \n",
        "          if test_f == False:\n",
        "            text_file = open(\"wrong_preds.txt\", \"a\")\n",
        "            text_file.write(test_ip_transcription[t_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "            text_file.close()            \n",
        "\n",
        "    accuracy_ = float(correct_predictions)/float(batch_size)\n",
        "    return accuracy_    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is called in wandb agent and this is responsible for compiling and training the model."
      ],
      "metadata": {
        "id": "c79dHcANLnRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBZpDrZ0O3AQ"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "  wandb.init()\n",
        "  rnn_type=wandb.config.rnn_type\n",
        "  embedding_dim=wandb.config.embedding_dim\n",
        "  latent_dim=wandb.config.latent_dim\n",
        "  enc_layer=wandb.config.enc_layer\n",
        "  dec_layer=wandb.config.dec_layer\n",
        "  dropout=wandb.config.dropout\n",
        "  epochs=wandb.config.epochs\n",
        "  bs=wandb.config.bs\n",
        "  wandb.run.name = 'epochs_'+str(epochs)+'_bs_'+str(bs)+'_rnn_type_'+str(rnn_type)+'_em_'+str(embedding_dim)+'_latd_'+str(latent_dim)+'_encs_'+str(enc_layer)+'_decs_'+str(dec_layer)+'_dr_'+str(dropout)\n",
        "\n",
        "\n",
        "  model=seq_seq_model(rnn_type=rnn_type,embed_dim=embedding_dim,encoder_layers=enc_layer,decoder_layers=dec_layer,dropout=dropout)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  for i in range(epochs):\n",
        "    hist=model.fit(\n",
        "        [train_input_text, train_output_text],\n",
        "        tf.concat([train_output_text[:,1:],tf.zeros((train_output_text[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=1,shuffle=True\n",
        "    )\n",
        "    # Save model\n",
        "    model.save(\"s2s.keras\")\n",
        "    # Run inferencing\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "    encoder_model,decoder_model=inference_(inf,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "    #log train loss to wandb\n",
        "    wandb.log({\"train_loss\": hist.history['loss'][0]})\n",
        "    val_acc=decode_and_eval(rnn_type,val_input_text,encoder_model,decoder_model,val_input_text.shape[0],enc_layer,dec_layer,False)\n",
        "    wandb.log({\"val_acc\":val_acc})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By changing the sweep_config variable you can select the various parameter values to experiment with"
      ],
      "metadata": {
        "id": "CO1VYgnrLewp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ07aLFXRy5R"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Assignment_3_partA_all_params\",\n",
        "    \"description\": \"Assignment_3_partA_all_params\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\" : {\n",
        "        \"name\" : \"val_acc\",\n",
        "        \"goal\" : \"maximize\"\n",
        "    },\n",
        "    \"project\": \"CS6910_Assignment3\",\n",
        "    \"parameters\": {\n",
        "        \"rnn_type\": {\n",
        "            \"values\": [\"LSTM\",\"GRU\",\"RNN\"]\n",
        "        },\n",
        "        \"embedding_dim\": {\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"enc_layer\": {\n",
        "            \"values\": [3,4,5]\n",
        "        },\n",
        "        \"dec_layer\" : {\n",
        "            \"values\" : [2,3,5]\n",
        "        },\n",
        "        \"latent_dim\" : {\n",
        "            \"values\" :[200,256,512]\n",
        "        },\n",
        "        \"bs\":{\n",
        "            \"values\":[64,100,200]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2,0.3]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [10]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921,
          "referenced_widgets": [
            "ee70a42fc1294a40bffe196bb2c97f72",
            "b3095f1114f74341859d8ee6fa5b4216",
            "07bfa47afdc943b093d2c1b75f81dbd4",
            "ba6713065ae544b6b1ca4e27880bcc20",
            "b9ca5dca32064019bc8b881214231991",
            "a7ec3aeeb7af4a23847e3ad39c5e08c4",
            "9fa17aa547d54e5a8b87c4dd0865ff84",
            "eb17601941cb4a35b44a038cd6b609fd"
          ]
        },
        "id": "9pjGOK7KPXKR",
        "outputId": "6cde9e51-ca7a-4568-a7b1-5862c0b0345d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ji0k78a with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbs: 200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layer: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layer: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: GRU\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220508_135721-8ji0k78a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/runs/8ji0k78a\" target=\"_blank\">lilac-sweep-78</a></strong> to <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/sweeps/tjot1usa\" target=\"_blank\">https://wandb.ai/swe-rana/CS6910_Assignment3/sweeps/tjot1usa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "342/342 [==============================] - 99s 245ms/step - loss: 0.9857 - accuracy: 0.7309\n",
            "342/342 [==============================] - 84s 245ms/step - loss: 0.6265 - accuracy: 0.8137\n",
            "342/342 [==============================] - 84s 245ms/step - loss: 0.4061 - accuracy: 0.8789\n",
            "342/342 [==============================] - 84s 245ms/step - loss: 0.2362 - accuracy: 0.9322\n",
            "342/342 [==============================] - 84s 246ms/step - loss: 0.1415 - accuracy: 0.9608\n",
            "342/342 [==============================] - 84s 245ms/step - loss: 0.0987 - accuracy: 0.9728\n",
            "342/342 [==============================] - 84s 245ms/step - loss: 0.0743 - accuracy: 0.9794\n",
            "342/342 [==============================] - 84s 245ms/step - loss: 0.0586 - accuracy: 0.9836\n",
            "342/342 [==============================] - 83s 243ms/step - loss: 0.0479 - accuracy: 0.9865\n",
            "342/342 [==============================] - 83s 243ms/step - loss: 0.0399 - accuracy: 0.9885\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.092 MB of 0.092 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee70a42fc1294a40bffe196bb2c97f72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▅▄▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▅▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.03992</td></tr><tr><td>val_acc</td><td>0.48308</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">lilac-sweep-78</strong>: <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/runs/8ji0k78a\" target=\"_blank\">https://wandb.ai/swe-rana/CS6910_Assignment3/runs/8ji0k78a</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220508_135721-8ji0k78a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dlj0z060 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbs: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layer: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layer: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: GRU\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220508_142746-dlj0z060</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/runs/dlj0z060\" target=\"_blank\">icy-sweep-77</a></strong> to <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/sweeps/tjot1usa\" target=\"_blank\">https://wandb.ai/swe-rana/CS6910_Assignment3/sweeps/tjot1usa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1066/1066 [==============================] - 79s 64ms/step - loss: 0.7898 - accuracy: 0.7733\n",
            "1066/1066 [==============================] - 68s 64ms/step - loss: 0.4505 - accuracy: 0.8652\n",
            "1066/1066 [==============================] - 68s 64ms/step - loss: 0.2367 - accuracy: 0.9318\n",
            "1002/1066 [===========================>..] - ETA: 4s - loss: 0.1501 - accuracy: 0.9578"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "#sweep_id = wandb.sweep(sweep_config,entity=\"swe-rana\",project=\"CS6910_Assignment3\")  # please uncomment this line to generate your own sweep id\n",
        "#wandb.agent(sweep_id,train,entity=\"swe-rana\",project=\"CS6910_Assignment3\") # uncomment this line to run with your own sweep Id\n",
        "\n",
        "wandb.agent(\"tjot1usa\",train,entity=\"swe-rana\",project=\"CS6910_Assignment3\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rnn_seq_to_seq.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee70a42fc1294a40bffe196bb2c97f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3095f1114f74341859d8ee6fa5b4216",
              "IPY_MODEL_07bfa47afdc943b093d2c1b75f81dbd4"
            ],
            "layout": "IPY_MODEL_ba6713065ae544b6b1ca4e27880bcc20"
          }
        },
        "b3095f1114f74341859d8ee6fa5b4216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ca5dca32064019bc8b881214231991",
            "placeholder": "​",
            "style": "IPY_MODEL_a7ec3aeeb7af4a23847e3ad39c5e08c4",
            "value": "0.143 MB of 0.143 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "07bfa47afdc943b093d2c1b75f81dbd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa17aa547d54e5a8b87c4dd0865ff84",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb17601941cb4a35b44a038cd6b609fd",
            "value": 1
          }
        },
        "ba6713065ae544b6b1ca4e27880bcc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ca5dca32064019bc8b881214231991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ec3aeeb7af4a23847e3ad39c5e08c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fa17aa547d54e5a8b87c4dd0865ff84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb17601941cb4a35b44a038cd6b609fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}