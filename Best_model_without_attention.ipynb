{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Best_model_without_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59797b6447ef4d41b720bdcba12f87e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_741d311e0c644aa4a1ae07eee4784a4b",
              "IPY_MODEL_4bb6e8dab41c4c4f888e2ba36c33a544"
            ],
            "layout": "IPY_MODEL_517b73f96d0c4339af45b1cd5347fd9c"
          }
        },
        "741d311e0c644aa4a1ae07eee4784a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2d46656498d45bcbb6629ba5d796fee",
            "placeholder": "​",
            "style": "IPY_MODEL_c59ad94002824589bfbb12192b7645b6",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "4bb6e8dab41c4c4f888e2ba36c33a544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de43afd6dc454fa39c545e86aeb96a13",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e63892062114828b792137040d26b62",
            "value": 1
          }
        },
        "517b73f96d0c4339af45b1cd5347fd9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d46656498d45bcbb6629ba5d796fee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59ad94002824589bfbb12192b7645b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de43afd6dc454fa39c545e86aeb96a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e63892062114828b792137040d26b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKFI4WuM9HwP",
        "outputId": "869e5fbd-7375-4514-e36f-852faf85dbad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 952 kB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=11dd83d5053803a7f2fe1059fda943e935ddc7c4a459bf00ad7a5b475ba513c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MAwasVBLpY4q"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4MqSm5SqI6H",
        "outputId": "7c021454-0aff-4f56-f26c-a1ae598dec4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   106M      0  0:00:18  0:00:18 --:--:--  103M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!tar -xvf  'daksh.tar'"
      ],
      "metadata": {
        "id": "zX8Dg-AsqKLB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if input_tokenizer is None:\n",
        "      df=df.sample(frac=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  #only train set will have input_tokenizer as none. Validation and test will will use the same.\n",
        "  if input_tokenizer is None:\n",
        "    input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "  input_tensor = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,padding='post')\n",
        "  if target_tokenizer is None:\n",
        "    target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "  #tokenize the text\n",
        "  target_tensor = target_tokenizer.texts_to_sequences(target_texts)\n",
        "  #pad the text\n",
        "  target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post')\n",
        "  #for dataset which is not training we pad to make maximum length same as train set.\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ],
      "metadata": {
        "id": "r8njwOEOqOpE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer=data(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\")\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])"
      ],
      "metadata": {
        "id": "ZBg1LTwTqQes"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  input_tensor.shape[1]\n",
        "max_decoder_seq_length = target_tensor.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "#convert index to character\n",
        "index_to_char_input = dict((input_tokenizer.word_index[key], key) for key in input_tokenizer.word_index.keys())\n",
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())\n",
        "\n"
      ],
      "metadata": {
        "id": "TjGycGeZqbKA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout):\n",
        "  #input layer ; takes in tokenize input\n",
        "  encoder_inputs = keras.Input(shape=( max_encoder_seq_length))\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "  #will store output of last added layer so that we can add multiple layers\n",
        "  last_encoder=None\n",
        "  if rnn_type=='LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.LSTM(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last LSTM Layer\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one LSTM layer.The above loop won't run.\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state_h, state_c = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state_h, state_c = encoder(last_encoder)\n",
        "    #storing the hidden states only\n",
        "    encoder_states = [state_h, state_c]\n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.GRU(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last GRU Layer\n",
        "    encoder = keras.layers.GRU(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one GRU layer.The above loop won't run\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(encoder_layers-1):      \n",
        "      encoder = keras.layers.SimpleRNN(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last RNN Layer\n",
        "    encoder = keras.layers.SimpleRNN(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one RNN layer.The above loop won't run\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]  \n",
        "\n",
        "\n",
        "  decoder_inputs = keras.Input(shape=( max_decoder_seq_length))\n",
        "  embed = keras.layers.Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
        "\n",
        "  if rnn_type==\"LSTM\":\n",
        "    #add all the LSTM layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _, _ = decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  elif rnn_type==\"GRU\":\n",
        "    #add all the GRU layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  elif rnn_type==\"RNN\":\n",
        "    #add all the RNN layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.SimpleRNN(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  #specifying model inputs and outputs.\n",
        "  # encoder_inputs -> Input to encoder\n",
        "  # decoder_inputs -> Input to decoder for teacher forcing\n",
        "  # decoder_outputs -> Output\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "M3Do666VqhLN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def build_inference(model,encoder_layers,decoder_layers):\n",
        "    encoder_inputs = model.input[0]  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "    decoder_inputs =  keras.Input(shape=( 1))  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        #every layer must have an input through which we can supply it's hidden state\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "        x = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input_h)\n",
        "        decoder_states_inputs.append (decoder_state_input_c)\n",
        "        decoder_states.append (state_h_dec)\n",
        "        decoder_states.append (state_c_dec)\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      decoder_states_inputs=[] #Contain all input layers for different GRU's hidden state\n",
        "      decoder_states=[] #Contains the hidden states\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)      \n",
        "    decoder_dense = model.get_layer('final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "    return encoder_model,decoder_model\n"
      ],
      "metadata": {
        "id": "Dk1zviZQqkTc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_batch(rnn_type,input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    if rnn_type=='GRU' or 'RNN':\n",
        "      states_value=[states_value]\n",
        "    nl=states_value\n",
        "    for i in range(decoder_layers-1):\n",
        "      nl=nl+states_value\n",
        "    states_value=nl\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        out = decoder_model.predict(tuple([prev_char_index] + states_value))\n",
        "        output_probability=out[0]\n",
        "        states_value = out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words\n"
      ],
      "metadata": {
        "id": "r4gPyrQtqnXX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #Get all the predicted words\n",
        "  pred=decode_batch(\"GRU\",test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  return float(success)/float(test_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "XTMwp9eOqo-o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_validate(encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #get all the predicted words\n",
        "  pred=decode_batch(\"GRU\",val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=val_target_texts[seq_index][1:-1]\n",
        "      #test the words one by one\n",
        "      if predicted_word == target_word:\n",
        "        success+=1\n",
        "  return float(success)/float(val_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "Q1pqCRq3qwq4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  global latent_dim\n",
        "  latent_dim = 256\n",
        "  global epochs\n",
        "  epochs = 10\n",
        "  model=build_model(rnn_type=\"GRU\",embedding_dim=64,encoder_layers=3,decoder_layers=3,dropout=0.3)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  hist=model.fit(\n",
        "        [input_tensor, target_tensor],\n",
        "        tf.concat([target_tensor[:,1:],tf.zeros((target_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=100,\n",
        "        epochs=10,shuffle=True\n",
        "  )\n",
        "  # Save model\n",
        "  model.save(\"s2s.keras\")\n",
        "  # Restore the model and construct the encoder and decoder.\n",
        "  inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "  encoder_model,decoder_model=build_inference(inf,encoder_layers=3,decoder_layers=3)\n",
        "  #log train loss to wandb\n",
        "  val_acc=batch_validate(encoder_model,decoder_model,3,3)\n",
        "  encoder_model.save(\"enc_model\")\n",
        "  decoder_model.save(\"dec_model\")\n",
        "  model.save(\"seq2seq\")\n"
      ],
      "metadata": {
        "id": "JRLx677tqyMw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_model = train()"
      ],
      "metadata": {
        "id": "ueGh1kjHrDql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "encoder_model,decoder_model=build_inference(inf,encoder_layers=3,decoder_layers=3)\n",
        "encoder_model.save(\"enc_model\")\n",
        "decoder_model.save(\"dec_model\")\n"
      ],
      "metadata": {
        "id": "MklXd7nQvcTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "encoder_model = keras.models.load_model(\"/content/enc_model\")\n",
        "decoder_model = keras.models.load_model(\"/content/dec_model\")\n",
        "\n",
        "val_acc=batch_validate(encoder_model,decoder_model,3,3)\n"
      ],
      "metadata": {
        "id": "k7KWkGmV8iPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_accuracy=test_accuracy(encoder_model,decoder_model,3,3)"
      ],
      "metadata": {
        "id": "R3JwMap29NCS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Test_accuracy*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT_JMewI-A-C",
        "outputId": "5a956594-d850-48c3-aabf-9e4a745523ea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46.27039627039627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"CS6910_Assignment3\", entity=\"swe-rana\")\n",
        "wandb.log({\"Test accuracy\": Test_accuracy})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "59797b6447ef4d41b720bdcba12f87e4",
            "741d311e0c644aa4a1ae07eee4784a4b",
            "4bb6e8dab41c4c4f888e2ba36c33a544",
            "517b73f96d0c4339af45b1cd5347fd9c",
            "c2d46656498d45bcbb6629ba5d796fee",
            "c59ad94002824589bfbb12192b7645b6",
            "de43afd6dc454fa39c545e86aeb96a13",
            "9e63892062114828b792137040d26b62"
          ]
        },
        "id": "833OIPsD84wU",
        "outputId": "ecdb72c9-88a1-4478-9ef9-e77b51771444"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2fqfhsrc) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59797b6447ef4d41b720bdcba12f87e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">true-armadillo-620</strong>: <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/runs/2fqfhsrc\" target=\"_blank\">https://wandb.ai/swe-rana/CS6910_Assignment3/runs/2fqfhsrc</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220506_152522-2fqfhsrc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2fqfhsrc). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220506_152550-1sh9t1xr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3/runs/1sh9t1xr\" target=\"_blank\">snowy-cherry-621</a></strong> to <a href=\"https://wandb.ai/swe-rana/CS6910_Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}